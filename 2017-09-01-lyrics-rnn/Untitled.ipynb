{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement\n",
    "### This notebook code is based on Python3 and tensorflow 1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tensorflow.contrib import rnn as rnn_cell\n",
    "from tensorflow.contrib import legacy_seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class include the variables we might use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can write this code to a py.file called config.\n",
    "class Config():\n",
    "    \n",
    "    batch_size = 32\n",
    "    n_epoch = 100\n",
    "    # modify the learning_rate through time\n",
    "    learning_rate = 0.01\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    # grad_clip incase of gradient explosion\n",
    "    grad_clip = 5\n",
    "    # prob of dropout\n",
    "    keep_prob = 0.5\n",
    "\n",
    "    # state_size is hidden_size in BasicLSTMCell \n",
    "    # size of hidden layer of neurons\n",
    "    state_size = 100\n",
    "    # number of RNN layers\n",
    "    num_layers = 3\n",
    "    # length of sentence\n",
    "    seq_length = 20\n",
    "    log_dir = './logs'\n",
    "    metadata = 'metadata.tsv'\n",
    "    # num of chars to generate\n",
    "    gen_num = 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class tackle with the input data to get the standard input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can write this code to a py.file named datagenerator\n",
    "class DataGenerator():\n",
    "\n",
    "    def __init__(self, filename, config):\n",
    "        \n",
    "        # length of the sentence / \n",
    "        # number of words that input the network one-time\n",
    "        self.seq_length = config.seq_length \n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        # read the data\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Take every Chinese word as a word\n",
    "        self.words = list(set(self.data))\n",
    "        self.total_len,self.vocab_size= len(self.data),len(self.words)   \n",
    "        self.words.sort()\n",
    "        print ('data has %d characters, %d unique.' % (self.total_len, self.vocab_size)) \n",
    "        # word2index & index2word\n",
    "        self.char2id_dict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.id2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "        # pointer position to generate current batch\n",
    "        self.p = 0\n",
    "        # save metadata file\n",
    "        self.save_metadata(config.metadata)\n",
    "\n",
    "    def char2id(self, c):\n",
    "        return self.char2id_dict[c]\n",
    "\n",
    "    def id2char(self, id):\n",
    "        return self.id2char_dict[id]\n",
    "\n",
    "    # save the vocabulary with its index\n",
    "    def save_metadata(self, file):\n",
    "        with open(file, 'w') as f:\n",
    "            f.write('id\\tchar\\n')\n",
    "            for i in range(self.vocab_size):\n",
    "                c = self.id2char(i)\n",
    "                f.write('{}\\t{}\\n'.format(i, c))\n",
    "\n",
    "    # Get the train data and targets for every batch\n",
    "    def next_batch(self):\n",
    "        input_batches = []\n",
    "        target_batches = []\n",
    "        for i in range(self.batch_size):\n",
    "            # In case of that all the input data has been trained\n",
    "            if self.p + self.seq_length + 1 >= self.total_len:\n",
    "                # go from start of data\n",
    "                self.p = 0\n",
    "  \n",
    "            inputs = [self.char2id(ch) for ch in self.data[self.p : self.p + self.seq_length]]\n",
    "            targets = [self.char2id(ch) for ch in self.data[self.p + 1 : self.p + self.seq_length + 1]]\n",
    "            # update pointer position\n",
    "            self.p += self.seq_length  \n",
    "            input_batches.append(inputs)\n",
    "            target_batches.append(targets)\n",
    "        return input_batches, target_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class model( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, config, data, is_training=False):\n",
    "         \n",
    "        if not is_training:\n",
    "            config.batch_size = 1\n",
    "            config.seq_length = 1\n",
    "\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.input_data = tf.placeholder(\n",
    "                tf.int32, [config.batch_size, config.seq_length])\n",
    "            self.target_data = tf.placeholder(\n",
    "                tf.int32, [config.batch_size, config.seq_length])\n",
    "\n",
    "\n",
    "        with tf.name_scope('model'):\n",
    "        \n",
    "\n",
    "            def lstm_cell():\n",
    "                lstm_cell = rnn_cell.BasicLSTMCell(config.state_size)\n",
    "                if is_training and config.keep_prob < 1:\n",
    "                    lstm_cell = rnn_cell.DropoutWrapper(\n",
    "                        lstm_cell, output_keep_prob=config.keep_prob)\n",
    "                return lstm_cell\n",
    "            # Attention: it says you can't use [lstm_cell] * config.num_layers\n",
    "            self.cell = rnn_cell.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)])\n",
    "            # 构造完多层LSTM以后，使用zero_state即可对各种状态进行初始化。\n",
    "            self.initial_state = self.cell.zero_state(config.batch_size, tf.float32)\n",
    "\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                    # embedding means word embedding.\n",
    "                    # Turn the input words into embedding , \n",
    "                    # so the state_size is the size of embedding\n",
    "                embedding = tf.get_variable('embedding', [data.vocab_size, config.state_size])\n",
    "                # 返回一个tensor，shape是(batch_size, seq_length, state_size)\n",
    "                inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "            outputs, last_state = tf.nn.dynamic_rnn(\n",
    "                self.cell, inputs, initial_state=self.initial_state)\n",
    "\n",
    "            w = tf.get_variable('softmax_w', [config.state_size, data.vocab_size])\n",
    "            b = tf.get_variable('softmax_b', [data.vocab_size])\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # 把之前的list展开，成[batch, hidden_size*seq_length],\n",
    "            # 然后 reshape, 成[batch*seq_length, hidden_size]\n",
    "            output = tf.reshape(outputs, [-1, config.state_size])\n",
    "\n",
    "            self.logits = tf.matmul(output, w) + b \n",
    "            self.probs = tf.nn.softmax(self.logits) \n",
    "            self.last_state = last_state\n",
    "\n",
    "            # target, [batch_size, seq_length] 然后展开成一维列表\n",
    "            targets = tf.reshape(self.target_data, [-1])\n",
    "            # loss: shape=[batch*seq_length]\n",
    "            loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                                                    [targets],\n",
    "                                                    [tf.ones_like(targets, dtype=tf.float32)])\n",
    "            # 计算得到平均每批batch的误差\n",
    "            self.cost = tf.reduce_sum(loss) / config.batch_size\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "\n",
    "        with tf.name_scope('optimize'):\n",
    "            self.lr = tf.placeholder(tf.float32, [])\n",
    "            tf.summary.scalar('learning_rate', self.lr)\n",
    "\n",
    "            # 通过tf.trainable_variables 可以得到整个模型中所有trainable=True的Variable。\n",
    "            # 实际得到的tvars是一个列表，里面存有所有可以进行训练的变量。\n",
    "            tvars = tf.trainable_variables()\n",
    "            # tf.gradients 返回一个len(xs)的tesnor列表\n",
    "            grads = tf.gradients(self.cost, tvars)\n",
    "            # 梯度修剪，控制梯度爆炸\n",
    "            grads, _ = tf.clip_by_global_norm(grads, config.grad_clip)\n",
    "            for g in grads:\n",
    "                tf.summary.histogram(g.name, g)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            self.merged_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, model, config):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter(config.log_dir, sess.graph)\n",
    "        start_time = time.time()\n",
    "        costs = 0.0\n",
    "        iters = 0\n",
    "\n",
    "        # projector for tensorboard\n",
    "        Pro_con = projector.ProjectorConfig()\n",
    "        embed = Pro_con.embeddings.add()\n",
    "        embed.tensor_name = 'rnnlm/embedding:0'\n",
    "        embed.metadata_path = config.metadata\n",
    "        projector.visualize_embeddings(writer, Pro_con)\n",
    "\n",
    "        # n_epoch means the using times of the whole input_data\n",
    "        max_iter = config.n_epoch * \\\n",
    "            (data.total_len // config.seq_length) // config.batch_size\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            learning_rate = config.learning_rate * \\\n",
    "                (config.decay_rate ** (i // config.decay_steps))\n",
    "            x_batch, y_batch = data.next_batch()\n",
    "            feed_dict = {model.input_data: x_batch,\n",
    "                         model.target_data: y_batch, model.lr: learning_rate}\n",
    "            train_loss, summary, _, _ = sess.run([model.cost, model.merged_op, model.last_state, model.train_op],\n",
    "                                                 feed_dict)\n",
    "\n",
    "            # ------------------------ #\n",
    "            # still have some question #\n",
    "            # ------------------------ #\n",
    "            costs += train_loss\n",
    "            iters += config.seq_length\n",
    "            if i % 10 == 0:\n",
    "                writer.add_summary(summary, global_step=i)\n",
    "                print('Step:{}/{}, training_loss:{:4f},perplexity:{:2f},cost-time:{:2f}'\\\n",
    "                    .format(i,max_iter, train_loss,np.exp(train_loss / iters),(time.time() - start_time)))\n",
    "     \n",
    "            start_time = time.time()\n",
    "            if i % 2000 == 0 or (i + 1) == max_iter:\n",
    "                saver.save(sess, os.path.join(\n",
    "                    config.log_dir, 'lyrics_model.ckpt'), global_step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the words by network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.latest_checkpoint(args.log_dir)\n",
    "        print(ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        # initial phrase to warm RNN\n",
    "        prime = u'你要离开我知道很简单'\n",
    "        state = sess.run(model.cell.zero_state(1, tf.float32))\n",
    "\n",
    "        for word in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = data.char2id(word)\n",
    "            feed = {model.input_data: x, model.initial_state: state}\n",
    "            state = sess.run(model.last_state, feed)\n",
    "\n",
    "        word = prime[-1]\n",
    "        lyrics = prime\n",
    "        for i in range(args.gen_num):\n",
    "            x = np.zeros([1, 1])\n",
    "            x[0, 0] = data.char2id(word)\n",
    "            feed_dict = {model.input_data: x, model.initial_state: state}\n",
    "            probs, state = sess.run([model.probs, model.last_state], feed_dict)\n",
    "            p = probs[0]\n",
    "            word = data.id2char(np.argmax(p))\n",
    "            print(word, end='')\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.05)\n",
    "            lyrics += word\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(is_training):\n",
    "\n",
    "    config = Config()\n",
    "    data = DataGenerator('JayLyrics.txt', config)\n",
    "    model = Model(config, data, is_training=is_training)\n",
    "\n",
    "    run_fn = train if is_training else sample\n",
    "\n",
    "    run_fn(data, model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 65697 characters, 2636 unique.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_0:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_0_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_1:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_1_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_2:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_2_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_3:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_3_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_4:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_4_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_5:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_5_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_6:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_6_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_7:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_7_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_8:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_8_0 instead.\n",
      "Step:0/10262, training_loss:157.494110,perplexity:2629.911581,cost-time:0.460443\n",
      "Step:10/10262, training_loss:128.074905,perplexity:1.789898,cost-time:0.219037\n",
      "Step:20/10262, training_loss:131.998962,perplexity:1.369278,cost-time:0.220903\n",
      "Step:30/10262, training_loss:124.245628,perplexity:1.221887,cost-time:0.267117\n",
      "Step:40/10262, training_loss:133.415649,perplexity:1.176686,cost-time:0.222798\n",
      "Step:50/10262, training_loss:118.875732,perplexity:1.123608,cost-time:0.210993\n",
      "Step:60/10262, training_loss:138.995895,perplexity:1.120675,cost-time:0.233490\n",
      "Step:70/10262, training_loss:128.026352,perplexity:1.094349,cost-time:0.227825\n",
      "Step:80/10262, training_loss:129.411743,perplexity:1.083161,cost-time:0.221837\n",
      "Step:90/10262, training_loss:124.616486,perplexity:1.070869,cost-time:0.221562\n",
      "Step:100/10262, training_loss:131.523849,perplexity:1.067277,cost-time:0.224467\n",
      "Step:110/10262, training_loss:128.418259,perplexity:1.059552,cost-time:0.221841\n",
      "Step:120/10262, training_loss:118.516769,perplexity:1.050193,cost-time:0.224173\n",
      "Step:130/10262, training_loss:130.556473,perplexity:1.051093,cost-time:0.233479\n",
      "Step:140/10262, training_loss:128.462280,perplexity:1.046608,cost-time:0.268094\n",
      "Step:150/10262, training_loss:125.661163,perplexity:1.042487,cost-time:0.209932\n",
      "Step:160/10262, training_loss:139.781845,perplexity:1.044367,cost-time:0.215292\n",
      "Step:170/10262, training_loss:129.083435,perplexity:1.038465,cost-time:0.277865\n",
      "Step:180/10262, training_loss:140.833984,perplexity:1.039671,cost-time:0.224353\n",
      "Step:190/10262, training_loss:127.293152,perplexity:1.033884,cost-time:0.270840\n",
      "Step:200/10262, training_loss:116.829712,perplexity:1.029489,cost-time:0.230868\n",
      "Step:210/10262, training_loss:127.680832,perplexity:1.030718,cost-time:0.239986\n",
      "Step:220/10262, training_loss:123.420303,perplexity:1.028317,cost-time:0.332901\n",
      "Step:230/10262, training_loss:135.978836,perplexity:1.029870,cost-time:0.234828\n",
      "Step:240/10262, training_loss:126.733307,perplexity:1.026642,cost-time:0.232981\n",
      "Step:250/10262, training_loss:128.186447,perplexity:1.025864,cost-time:0.238195\n",
      "Step:260/10262, training_loss:124.894135,perplexity:1.024215,cost-time:0.246074\n",
      "Step:270/10262, training_loss:121.544144,perplexity:1.022678,cost-time:0.306835\n",
      "Step:280/10262, training_loss:125.320740,perplexity:1.022550,cost-time:0.226550\n",
      "Step:290/10262, training_loss:114.623421,perplexity:1.019890,cost-time:0.227077\n",
      "Step:300/10262, training_loss:118.388397,perplexity:1.019860,cost-time:0.255464\n",
      "Step:310/10262, training_loss:127.103043,perplexity:1.020645,cost-time:0.224352\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9d335f479abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-686f54fe2b5a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(is_training)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrun_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5f63753991b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m                          model.target_data: y_batch, model.lr: learning_rate}\n\u001b[1;32m     25\u001b[0m             train_loss, summary, _, _ = sess.run([model.cost, model.merged_op, model.last_state, model.train_op],\n\u001b[0;32m---> 26\u001b[0;31m                                                  feed_dict)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# ------------------------ #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
