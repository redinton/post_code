{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement\n",
    "### This notebook code is based on Python3 and tensorflow 1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tensorflow.contrib import rnn as rnn_cell\n",
    "from tensorflow.contrib import legacy_seq2seq as seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class include the variables we might use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can write this code to a py.file called config.\n",
    "class Config():\n",
    "    \n",
    "    batch_size = 32\n",
    "    n_epoch = 100\n",
    "    # modify the learning_rate through time\n",
    "    learning_rate = 0.01\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    # grad_clip incase of gradient explosion\n",
    "    grad_clip = 5\n",
    "    # prob of dropout\n",
    "    keep_prob = 0.5\n",
    "\n",
    "    # state_size is hidden_size in BasicLSTMCell \n",
    "    # size of hidden layer of neurons\n",
    "    state_size = 100\n",
    "    # number of RNN layers\n",
    "    num_layers = 3\n",
    "    # length of sentence\n",
    "    seq_length = 20\n",
    "    log_dir = './logs'\n",
    "    metadata = 'metadata.tsv'\n",
    "    # num of chars to generate\n",
    "    gen_num = 500 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class tackle with the input data to get the standard input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can write this code to a py.file named datagenerator\n",
    "class DataGenerator():\n",
    "3\n",
    "    def __init__(self, filename, config):\n",
    "        \n",
    "        # length of the sentence / \n",
    "        # number of words that input the network one-time\n",
    "        self.seq_length = config.seq_length \n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        # read the data\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "\n",
    "        # Take every Chinese word as a word\n",
    "        self.words = list(set(self.data))\n",
    "        self.total_len,self.vocab_size= len(self.data),len(self.words)   \n",
    "        self.words.sort()\n",
    "        print ('data has %d characters, %d unique.' % (self.total_len, self.vocab_size)) \n",
    "        # word2index & index2word\n",
    "        self.char2id_dict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.id2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "        # pointer position to generate current batch\n",
    "        self.p = 0\n",
    "        # save metadata file\n",
    "        self.save_metadata(config.metadata)\n",
    "\n",
    "    def char2id(self, c):\n",
    "        return self.char2id_dict[c]\n",
    "\n",
    "    def id2char(self, id):\n",
    "        return self.id2char_dict[id]\n",
    "\n",
    "    # save the vocabulary with its index\n",
    "    def save_metadata(self, file):\n",
    "        with open(file, 'w') as f:\n",
    "            f.write('id\\tchar\\n')\n",
    "            for i in range(self.vocab_size):\n",
    "                c = self.id2char(i)\n",
    "                f.write('{}\\t{}\\n'.format(i, c))\n",
    "\n",
    "    # Get the train data and targets for every batch\n",
    "    def next_batch(self):\n",
    "        input_batches = []\n",
    "        target_batches = []\n",
    "        for i in range(self.batch_size):\n",
    "            # In case of that all the input data has been trained\n",
    "            if self.p + self.seq_length + 1 >= self.total_len:\n",
    "                # go from start of data\n",
    "                self.p = 0\n",
    "  \n",
    "            inputs = [self.char2id(ch) for ch in self.data[self.p : self.p + self.seq_length]]\n",
    "            targets = [self.char2id(ch) for ch in self.data[self.p + 1 : self.p + self.seq_length + 1]]\n",
    "            # update pointer position\n",
    "            self.p += self.seq_length  \n",
    "            input_batches.append(inputs)\n",
    "            target_batches.append(targets)\n",
    "        return input_batches, target_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class model( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, config, data, is_training=False):\n",
    "         \n",
    "        if not is_training:\n",
    "            config.batch_size = 1\n",
    "            config.seq_length = 1\n",
    "\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.input_data = tf.placeholder(\n",
    "                tf.int32, [config.batch_size, config.seq_length])\n",
    "            self.target_data = tf.placeholder(\n",
    "                tf.int32, [config.batch_size, config.seq_length])\n",
    "\n",
    "\n",
    "        with tf.name_scope('model'):\n",
    "        \n",
    "            def lstm_cell():\n",
    "                lstm_cell = rnn_cell.BasicLSTMCell(config.state_size)\n",
    "                if is_training and config.keep_prob < 1:\n",
    "                    lstm_cell = rnn_cell.DropoutWrapper(\n",
    "                        lstm_cell, output_keep_prob=config.keep_prob)\n",
    "                return lstm_cell\n",
    "            # Attention: it says you can't use [lstm_cell] * config.num_layers\n",
    "            self.cell = rnn_cell.MultiRNNCell([lstm_cell() for _ in range(config.num_layers)])\n",
    "            # 构造完多层LSTM以后，使用zero_state即可对各种状态进行初始化。\n",
    "            self.initial_state = self.cell.zero_state(config.batch_size, tf.float32)\n",
    "\n",
    "\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                    # embedding means word embedding.\n",
    "                    # Turn the input words into embedding , \n",
    "                    # so the state_size is the size of embedding\n",
    "                embedding = tf.get_variable('embedding', [data.vocab_size, config.state_size])\n",
    "                # 返回一个tensor，shape是(batch_size, seq_length, state_size)\n",
    "                inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
    "\n",
    "            outputs, last_state = tf.nn.dynamic_rnn(\n",
    "                self.cell, inputs, initial_state=self.initial_state)\n",
    "\n",
    "            w = tf.get_variable('softmax_w', [config.state_size, data.vocab_size])\n",
    "            b = tf.get_variable('softmax_b', [data.vocab_size])\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # 把之前的list展开，成[batch, hidden_size*seq_length],\n",
    "            # 然后 reshape, 成[batch*seq_length, hidden_size]\n",
    "            output = tf.reshape(outputs, [-1, config.state_size])\n",
    "\n",
    "            self.logits = tf.matmul(output, w) + b \n",
    "            self.probs = tf.nn.softmax(self.logits) \n",
    "            self.last_state = last_state\n",
    "\n",
    "            # target, [batch_size, seq_length] 然后展开成一维列表\n",
    "            targets = tf.reshape(self.target_data, [-1])\n",
    "            # loss: shape=[batch*seq_length]\n",
    "            loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "                                                    [targets],\n",
    "                                                    [tf.ones_like(targets, dtype=tf.float32)])\n",
    "            # 计算得到平均每批batch的误差\n",
    "            self.cost = tf.reduce_sum(loss) / config.batch_size\n",
    "            tf.summary.scalar('loss', self.cost)\n",
    "\n",
    "        with tf.name_scope('optimize'):\n",
    "            self.lr = tf.placeholder(tf.float32, [])\n",
    "            tf.summary.scalar('learning_rate', self.lr)\n",
    "\n",
    "            # 通过tf.trainable_variables 可以得到整个模型中所有trainable=True的Variable。\n",
    "            # 实际得到的tvars是一个列表，里面存有所有可以进行训练的变量。\n",
    "            tvars = tf.trainable_variables()\n",
    "            # tf.gradients 返回一个len(xs)的tesnor列表\n",
    "            grads = tf.gradients(self.cost, tvars)\n",
    "            # 梯度修剪，控制梯度爆炸\n",
    "            grads, _ = tf.clip_by_global_norm(grads, config.grad_clip)\n",
    "            for g in grads:\n",
    "                tf.summary.histogram(g.name, g)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "            self.merged_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, model, config):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        writer = tf.summary.FileWriter(config.log_dir, sess.graph)\n",
    "        start_time = time.time()\n",
    "        costs = 0.0\n",
    "        iters = 0\n",
    "\n",
    "        # projector for tensorboard\n",
    "        Pro_con = projector.ProjectorConfig()\n",
    "        embed = Pro_con.embeddings.add()\n",
    "        embed.tensor_name = 'rnnlm/embedding:0'\n",
    "        embed.metadata_path = config.metadata\n",
    "        projector.visualize_embeddings(writer, Pro_con)\n",
    "\n",
    "        # n_epoch means the using times of the whole input_data\n",
    "        max_iter = config.n_epoch * \\\n",
    "            (data.total_len // config.seq_length) // config.batch_size\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            learning_rate = config.learning_rate * \\\n",
    "                (config.decay_rate ** (i // config.decay_steps))\n",
    "            x_batch, y_batch = data.next_batch()\n",
    "            feed_dict = {model.input_data: x_batch,\n",
    "                         model.target_data: y_batch, model.lr: learning_rate}\n",
    "            train_loss, summary, _, _ = sess.run([model.cost, model.merged_op, model.last_state, model.train_op],\n",
    "                                                 feed_dict)\n",
    "\n",
    "            # ------------------------ #\n",
    "            # still have some question #\n",
    "            # ------------------------ #\n",
    "            costs += train_loss\n",
    "            iters += config.seq_length\n",
    "            if i % 10 == 0:\n",
    "                writer.add_summary(summary, global_step=i)\n",
    "                print('Step:{}/{}, training_loss:{:4f},perplexity:{:2f},cost-time:{:2f}'\\\n",
    "                    .format(i,max_iter, train_loss,np.exp(train_loss / iters),(time.time() - start_time)))\n",
    "     \n",
    "            start_time = time.time()\n",
    "            if i % 2000 == 0 or (i + 1) == max_iter:\n",
    "                saver.save(sess, os.path.join(\n",
    "                    config.log_dir, 'lyrics_model.ckpt'), global_step=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the words by network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.latest_checkpoint(args.log_dir)\n",
    "        print(ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        # initial phrase to warm RNN\n",
    "        prime = u'你要离开我知道很简单'\n",
    "        state = sess.run(model.cell.zero_state(1, tf.float32))\n",
    "\n",
    "        for word in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = data.char2id(word)\n",
    "            feed = {model.input_data: x, model.initial_state: state}\n",
    "            state = sess.run(model.last_state, feed)\n",
    "\n",
    "        word = prime[-1]\n",
    "        lyrics = prime\n",
    "        for i in range(args.gen_num):\n",
    "            x = np.zeros([1, 1])\n",
    "            x[0, 0] = data.char2id(word)\n",
    "            feed_dict = {model.input_data: x, model.initial_state: state}\n",
    "            probs, state = sess.run([model.probs, model.last_state], feed_dict)\n",
    "            p = probs[0]\n",
    "            word = data.id2char(np.argmax(p))\n",
    "            print(word, end='')\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(0.05)\n",
    "            lyrics += word\n",
    "        return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(is_training):\n",
    "\n",
    "    config = Config()\n",
    "    data = DataGenerator('JayLyrics.txt', config)\n",
    "    model = Model(config, data, is_training=is_training)\n",
    "\n",
    "    run_fn = train if is_training else sample\n",
    "\n",
    "    run_fn(data, model,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 65697 characters, 2636 unique.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_0:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_0_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_1:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_1_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_2:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_2_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_3:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_3_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_4:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_4_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_5:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_5_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_6:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_6_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_7:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_7_0 instead.\n",
      "INFO:tensorflow:Summary name optimize/clip_by_global_norm/optimize/clip_by_global_norm/_8:0 is illegal; using optimize/clip_by_global_norm/optimize/clip_by_global_norm/_8_0 instead.\n",
      "Step:0/10262, training_loss:157.562622,perplexity:2638.936050,cost-time:0.469265\n",
      "Step:10/10262, training_loss:128.166687,perplexity:1.790645,cost-time:0.215064\n",
      "Step:20/10262, training_loss:129.691254,perplexity:1.361775,cost-time:0.237996\n",
      "Step:30/10262, training_loss:122.316101,perplexity:1.218090,cost-time:0.220353\n",
      "Step:40/10262, training_loss:134.529053,perplexity:1.178285,cost-time:0.230717\n",
      "Step:50/10262, training_loss:119.141464,perplexity:1.123901,cost-time:0.231313\n",
      "Step:60/10262, training_loss:138.858368,perplexity:1.120549,cost-time:0.219455\n",
      "Step:70/10262, training_loss:127.321144,perplexity:1.093805,cost-time:0.226363\n",
      "Step:80/10262, training_loss:127.401962,perplexity:1.081818,cost-time:0.230092\n",
      "Step:90/10262, training_loss:125.618935,perplexity:1.071459,cost-time:0.222531\n",
      "Step:100/10262, training_loss:132.210754,perplexity:1.067640,cost-time:0.220287\n",
      "Step:110/10262, training_loss:128.544067,perplexity:1.059612,cost-time:0.239094\n",
      "Step:120/10262, training_loss:118.606491,perplexity:1.050232,cost-time:0.248547\n",
      "Step:130/10262, training_loss:132.616058,perplexity:1.051920,cost-time:0.230910\n",
      "Step:140/10262, training_loss:130.134445,perplexity:1.047228,cost-time:0.238729\n",
      "Step:150/10262, training_loss:127.420586,perplexity:1.043095,cost-time:0.250385\n",
      "Step:160/10262, training_loss:144.065247,perplexity:1.045757,cost-time:0.241094\n",
      "Step:170/10262, training_loss:128.254684,perplexity:1.038213,cost-time:0.210204\n",
      "Step:180/10262, training_loss:140.576141,perplexity:1.039597,cost-time:0.218596\n",
      "Step:190/10262, training_loss:127.247948,perplexity:1.033872,cost-time:0.217843\n",
      "Step:200/10262, training_loss:117.051994,perplexity:1.029545,cost-time:0.213154\n",
      "Step:210/10262, training_loss:129.047775,perplexity:1.031052,cost-time:0.230955\n",
      "Step:220/10262, training_loss:122.706665,perplexity:1.028151,cost-time:0.226986\n",
      "Step:230/10262, training_loss:134.568130,perplexity:1.029556,cost-time:0.230411\n",
      "Step:240/10262, training_loss:127.536095,perplexity:1.026813,cost-time:0.227091\n",
      "Step:250/10262, training_loss:131.284271,perplexity:1.026497,cost-time:0.217491\n",
      "Step:260/10262, training_loss:125.580292,perplexity:1.024349,cost-time:0.235846\n",
      "Step:270/10262, training_loss:120.965759,perplexity:1.022569,cost-time:0.218450\n",
      "Step:280/10262, training_loss:125.848656,perplexity:1.022646,cost-time:0.225013\n",
      "Step:290/10262, training_loss:115.326317,perplexity:1.020013,cost-time:0.233752\n",
      "Step:300/10262, training_loss:117.902298,perplexity:1.019778,cost-time:0.228785\n",
      "Step:310/10262, training_loss:125.653870,perplexity:1.020407,cost-time:0.222539\n",
      "Step:320/10262, training_loss:126.029289,perplexity:1.019825,cost-time:0.229839\n",
      "Step:330/10262, training_loss:127.531090,perplexity:1.019451,cost-time:0.224081\n",
      "Step:340/10262, training_loss:126.238098,perplexity:1.018682,cost-time:0.228278\n",
      "Step:350/10262, training_loss:120.587326,perplexity:1.017326,cost-time:0.231166\n",
      "Step:360/10262, training_loss:119.025818,perplexity:1.016622,cost-time:0.224091\n",
      "Step:370/10262, training_loss:133.756027,perplexity:1.018190,cost-time:0.226185\n",
      "Step:380/10262, training_loss:114.664665,perplexity:1.015162,cost-time:0.235527\n",
      "Step:390/10262, training_loss:125.693489,perplexity:1.016203,cost-time:0.222200\n",
      "Step:400/10262, training_loss:125.243431,perplexity:1.015739,cost-time:0.227956\n",
      "Step:410/10262, training_loss:126.404709,perplexity:1.015497,cost-time:0.227209\n",
      "Step:420/10262, training_loss:117.602905,perplexity:1.014065,cost-time:0.214631\n",
      "Step:430/10262, training_loss:118.374924,perplexity:1.013827,cost-time:0.242470\n",
      "Step:440/10262, training_loss:114.888855,perplexity:1.013111,cost-time:0.235693\n",
      "Step:450/10262, training_loss:122.828568,perplexity:1.013710,cost-time:0.230081\n",
      "Step:460/10262, training_loss:114.765610,perplexity:1.012525,cost-time:0.251209\n",
      "Step:470/10262, training_loss:127.233795,perplexity:1.013598,cost-time:0.215210\n",
      "Step:480/10262, training_loss:118.603004,perplexity:1.012405,cost-time:0.228206\n",
      "Step:490/10262, training_loss:121.722748,perplexity:1.012473,cost-time:0.200978\n",
      "Step:500/10262, training_loss:118.969940,perplexity:1.011944,cost-time:0.253542\n",
      "Step:510/10262, training_loss:123.846619,perplexity:1.012192,cost-time:0.233446\n",
      "Step:520/10262, training_loss:121.550781,perplexity:1.011733,cost-time:0.215880\n",
      "Step:530/10262, training_loss:110.200226,perplexity:1.010431,cost-time:0.217971\n",
      "Step:540/10262, training_loss:117.548347,perplexity:1.010923,cost-time:0.218010\n",
      "Step:550/10262, training_loss:124.149719,perplexity:1.011330,cost-time:0.235896\n",
      "Step:560/10262, training_loss:117.136932,perplexity:1.010495,cost-time:0.235017\n",
      "Step:570/10262, training_loss:131.941864,perplexity:1.011621,cost-time:0.212439\n",
      "Step:580/10262, training_loss:119.152802,perplexity:1.010307,cost-time:0.222591\n",
      "Step:590/10262, training_loss:123.358612,perplexity:1.010491,cost-time:0.226173\n",
      "Step:600/10262, training_loss:119.990433,perplexity:1.010033,cost-time:0.226157\n",
      "Step:610/10262, training_loss:116.693428,perplexity:1.009595,cost-time:0.226048\n",
      "Step:620/10262, training_loss:118.936256,perplexity:1.009622,cost-time:0.226260\n",
      "Step:630/10262, training_loss:111.519295,perplexity:1.008876,cost-time:0.238633\n",
      "Step:640/10262, training_loss:130.443207,perplexity:1.010227,cost-time:0.227856\n",
      "Step:650/10262, training_loss:118.946198,perplexity:1.009178,cost-time:0.221615\n",
      "Step:660/10262, training_loss:119.671173,perplexity:1.009093,cost-time:0.211731\n",
      "Step:670/10262, training_loss:118.972237,perplexity:1.008905,cost-time:0.226271\n",
      "Step:680/10262, training_loss:118.352158,perplexity:1.008727,cost-time:0.240564\n",
      "Step:690/10262, training_loss:127.006943,perplexity:1.009232,cost-time:0.220814\n",
      "Step:700/10262, training_loss:116.405968,perplexity:1.008337,cost-time:0.248148\n",
      "Step:710/10262, training_loss:105.997665,perplexity:1.007482,cost-time:0.223684\n",
      "Step:720/10262, training_loss:120.255112,perplexity:1.008374,cost-time:0.232092\n",
      "Step:730/10262, training_loss:119.618668,perplexity:1.008215,cost-time:0.233633\n",
      "Step:740/10262, training_loss:113.043488,perplexity:1.007657,cost-time:0.231219\n",
      "Step:750/10262, training_loss:122.411209,perplexity:1.008183,cost-time:0.232199\n",
      "Step:760/10262, training_loss:119.736923,perplexity:1.007898,cost-time:0.227824\n",
      "Step:770/10262, training_loss:114.422440,perplexity:1.007448,cost-time:0.232092\n",
      "Step:780/10262, training_loss:120.680344,perplexity:1.007756,cost-time:0.230112\n",
      "Step:790/10262, training_loss:116.297333,perplexity:1.007378,cost-time:0.212799\n",
      "Step:800/10262, training_loss:116.641487,perplexity:1.007308,cost-time:0.227385\n",
      "Step:810/10262, training_loss:114.292206,perplexity:1.007071,cost-time:0.255174\n",
      "Step:820/10262, training_loss:118.166603,perplexity:1.007222,cost-time:0.230015\n",
      "Step:830/10262, training_loss:120.855179,perplexity:1.007298,cost-time:0.223230\n",
      "Step:840/10262, training_loss:119.652954,perplexity:1.007139,cost-time:0.263678\n",
      "Step:850/10262, training_loss:115.253769,perplexity:1.006795,cost-time:0.234781\n",
      "Step:860/10262, training_loss:118.253860,perplexity:1.006891,cost-time:0.223260\n",
      "Step:870/10262, training_loss:121.070526,perplexity:1.006974,cost-time:0.227571\n",
      "Step:880/10262, training_loss:123.147812,perplexity:1.007014,cost-time:0.222955\n",
      "Step:890/10262, training_loss:114.174057,perplexity:1.006428,cost-time:0.218739\n",
      "Step:900/10262, training_loss:117.075516,perplexity:1.006518,cost-time:0.225085\n",
      "Step:910/10262, training_loss:115.261726,perplexity:1.006346,cost-time:0.215707\n",
      "Step:920/10262, training_loss:128.699310,perplexity:1.007011,cost-time:0.230056\n",
      "Step:930/10262, training_loss:114.317459,perplexity:1.006158,cost-time:0.215558\n",
      "Step:940/10262, training_loss:116.798935,perplexity:1.006225,cost-time:0.218396\n",
      "Step:950/10262, training_loss:107.970779,perplexity:1.005693,cost-time:0.219189\n",
      "Step:960/10262, training_loss:115.900108,perplexity:1.006048,cost-time:0.226277\n",
      "Step:970/10262, training_loss:117.045212,perplexity:1.006045,cost-time:0.219298\n",
      "Step:980/10262, training_loss:117.372887,perplexity:1.006000,cost-time:0.237396\n",
      "Step:990/10262, training_loss:116.287445,perplexity:1.005884,cost-time:0.207724\n",
      "Step:1000/10262, training_loss:111.545853,perplexity:1.005587,cost-time:0.213831\n",
      "Step:1010/10262, training_loss:116.733902,perplexity:1.005790,cost-time:0.217714\n",
      "Step:1020/10262, training_loss:113.907242,perplexity:1.005594,cost-time:0.225691\n",
      "Step:1030/10262, training_loss:112.247398,perplexity:1.005458,cost-time:0.209947\n",
      "Step:1040/10262, training_loss:108.612305,perplexity:1.005230,cost-time:0.241621\n",
      "Step:1050/10262, training_loss:129.020691,perplexity:1.006157,cost-time:0.220506\n",
      "Step:1060/10262, training_loss:115.301414,perplexity:1.005448,cost-time:0.212693\n",
      "Step:1070/10262, training_loss:112.807907,perplexity:1.005280,cost-time:0.211109\n",
      "Step:1080/10262, training_loss:113.131279,perplexity:1.005246,cost-time:0.225862\n",
      "Step:1090/10262, training_loss:114.110611,perplexity:1.005243,cost-time:0.224576\n",
      "Step:1100/10262, training_loss:119.180008,perplexity:1.005427,cost-time:0.223413\n",
      "Step:1110/10262, training_loss:116.637329,perplexity:1.005263,cost-time:0.219581\n",
      "Step:1120/10262, training_loss:111.046371,perplexity:1.004965,cost-time:0.227319\n",
      "Step:1130/10262, training_loss:112.268188,perplexity:1.004976,cost-time:0.219314\n",
      "Step:1140/10262, training_loss:116.686134,perplexity:1.005126,cost-time:0.225735\n",
      "Step:1150/10262, training_loss:102.973785,perplexity:1.004483,cost-time:0.222394\n",
      "Step:1160/10262, training_loss:118.757378,perplexity:1.005128,cost-time:0.223966\n",
      "Step:1170/10262, training_loss:110.824173,perplexity:1.004743,cost-time:0.233827\n",
      "Step:1180/10262, training_loss:111.489624,perplexity:1.004731,cost-time:0.220733\n",
      "Step:1190/10262, training_loss:109.715149,perplexity:1.004617,cost-time:0.225412\n",
      "Step:1200/10262, training_loss:120.112091,perplexity:1.005013,cost-time:0.249428\n",
      "Step:1210/10262, training_loss:111.355713,perplexity:1.004608,cost-time:0.224483\n",
      "Step:1220/10262, training_loss:105.616821,perplexity:1.004334,cost-time:0.226691\n",
      "Step:1230/10262, training_loss:119.967361,perplexity:1.004885,cost-time:0.216896\n",
      "Step:1240/10262, training_loss:115.423355,perplexity:1.004661,cost-time:0.221156\n",
      "Step:1250/10262, training_loss:117.381348,perplexity:1.004703,cost-time:0.214822\n",
      "Step:1260/10262, training_loss:115.590981,perplexity:1.004594,cost-time:0.217031\n",
      "Step:1270/10262, training_loss:119.428368,perplexity:1.004709,cost-time:0.203263\n",
      "Step:1280/10262, training_loss:121.800507,perplexity:1.004765,cost-time:0.219463\n",
      "Step:1290/10262, training_loss:120.578377,perplexity:1.004681,cost-time:0.216664\n",
      "Step:1300/10262, training_loss:109.531799,perplexity:1.004218,cost-time:0.219210\n",
      "Step:1310/10262, training_loss:115.111923,perplexity:1.004400,cost-time:0.229272\n",
      "Step:1320/10262, training_loss:113.217194,perplexity:1.004294,cost-time:0.219311\n",
      "Step:1330/10262, training_loss:126.370201,perplexity:1.004758,cost-time:0.235718\n",
      "Step:1340/10262, training_loss:113.628372,perplexity:1.004246,cost-time:0.231719\n",
      "Step:1350/10262, training_loss:121.070663,perplexity:1.004491,cost-time:0.217170\n",
      "Step:1360/10262, training_loss:101.171928,perplexity:1.003724,cost-time:0.249300\n",
      "Step:1370/10262, training_loss:112.488495,perplexity:1.004111,cost-time:0.220017\n",
      "Step:1380/10262, training_loss:114.746048,perplexity:1.004163,cost-time:0.240725\n",
      "Step:1390/10262, training_loss:106.547394,perplexity:1.003837,cost-time:0.225815\n",
      "Step:1400/10262, training_loss:111.960480,perplexity:1.004004,cost-time:0.231395\n",
      "Step:1410/10262, training_loss:111.804794,perplexity:1.003970,cost-time:0.233187\n",
      "Step:1420/10262, training_loss:117.089279,perplexity:1.004128,cost-time:0.227764\n",
      "Step:1430/10262, training_loss:107.736145,perplexity:1.003771,cost-time:0.225375\n",
      "Step:1440/10262, training_loss:110.588181,perplexity:1.003845,cost-time:0.249274\n",
      "Step:1450/10262, training_loss:110.234550,perplexity:1.003806,cost-time:0.219572\n",
      "Step:1460/10262, training_loss:115.539307,perplexity:1.003962,cost-time:0.203618\n",
      "Step:1470/10262, training_loss:111.788979,perplexity:1.003807,cost-time:0.240776\n",
      "Step:1480/10262, training_loss:115.552490,perplexity:1.003909,cost-time:0.240248\n",
      "Step:1490/10262, training_loss:106.978477,perplexity:1.003594,cost-time:0.240843\n",
      "Step:1500/10262, training_loss:112.534225,perplexity:1.003756,cost-time:0.207974\n",
      "Step:1510/10262, training_loss:109.435425,perplexity:1.003628,cost-time:0.250520\n",
      "Step:1520/10262, training_loss:109.203522,perplexity:1.003596,cost-time:0.217780\n",
      "Step:1530/10262, training_loss:115.986214,perplexity:1.003795,cost-time:0.215171\n",
      "Step:1540/10262, training_loss:105.027321,perplexity:1.003414,cost-time:0.227521\n",
      "Step:1550/10262, training_loss:109.211639,perplexity:1.003527,cost-time:0.217326\n",
      "Step:1560/10262, training_loss:101.343765,perplexity:1.003251,cost-time:0.232951\n",
      "Step:1570/10262, training_loss:114.053406,perplexity:1.003637,cost-time:0.215298\n",
      "Step:1580/10262, training_loss:106.596298,perplexity:1.003377,cost-time:0.220018\n",
      "Step:1590/10262, training_loss:103.388672,perplexity:1.003254,cost-time:0.224012\n",
      "Step:1600/10262, training_loss:113.154434,perplexity:1.003540,cost-time:0.216418\n",
      "Step:1610/10262, training_loss:113.723785,perplexity:1.003536,cost-time:0.217870\n",
      "Step:1620/10262, training_loss:105.628891,perplexity:1.003263,cost-time:0.203325\n",
      "Step:1630/10262, training_loss:103.887688,perplexity:1.003190,cost-time:0.221874\n",
      "Step:1640/10262, training_loss:121.050766,perplexity:1.003695,cost-time:0.215554\n",
      "Step:1650/10262, training_loss:108.063866,perplexity:1.003278,cost-time:0.245686\n",
      "Step:1660/10262, training_loss:112.122284,perplexity:1.003381,cost-time:0.218899\n",
      "Step:1670/10262, training_loss:112.530128,perplexity:1.003373,cost-time:0.220484\n",
      "Step:1680/10262, training_loss:113.593773,perplexity:1.003384,cost-time:0.219933\n",
      "Step:1690/10262, training_loss:114.413925,perplexity:1.003389,cost-time:0.214304\n",
      "Step:1700/10262, training_loss:118.029221,perplexity:1.003475,cost-time:0.248335\n",
      "Step:1710/10262, training_loss:110.642372,perplexity:1.003238,cost-time:0.243031\n",
      "Step:1720/10262, training_loss:112.781364,perplexity:1.003282,cost-time:0.237103\n",
      "Step:1730/10262, training_loss:111.528999,perplexity:1.003227,cost-time:0.227731\n",
      "Step:1740/10262, training_loss:113.065048,perplexity:1.003252,cost-time:0.213028\n",
      "Step:1750/10262, training_loss:114.967186,perplexity:1.003288,cost-time:0.210567\n",
      "Step:1760/10262, training_loss:111.634094,perplexity:1.003175,cost-time:0.229889\n",
      "Step:1770/10262, training_loss:105.794296,perplexity:1.002991,cost-time:0.215208\n",
      "Step:1780/10262, training_loss:112.271286,perplexity:1.003157,cost-time:0.209192\n",
      "Step:1790/10262, training_loss:109.958328,perplexity:1.003074,cost-time:0.217425\n",
      "Step:1800/10262, training_loss:111.374191,perplexity:1.003097,cost-time:0.218335\n",
      "Step:1810/10262, training_loss:107.683975,perplexity:1.002977,cost-time:0.225156\n",
      "Step:1820/10262, training_loss:109.526268,perplexity:1.003012,cost-time:0.227238\n",
      "Step:1830/10262, training_loss:112.770111,perplexity:1.003084,cost-time:0.205795\n",
      "Step:1840/10262, training_loss:106.025871,perplexity:1.002884,cost-time:0.218248\n",
      "Step:1850/10262, training_loss:112.043442,perplexity:1.003031,cost-time:0.218115\n",
      "Step:1860/10262, training_loss:110.734787,perplexity:1.002980,cost-time:0.209205\n",
      "Step:1870/10262, training_loss:110.999908,perplexity:1.002971,cost-time:0.222259\n",
      "Step:1880/10262, training_loss:110.451675,perplexity:1.002940,cost-time:0.223858\n",
      "Step:1890/10262, training_loss:112.601738,perplexity:1.002982,cost-time:0.237718\n",
      "Step:1900/10262, training_loss:105.448814,perplexity:1.002777,cost-time:0.218765\n",
      "Step:1910/10262, training_loss:114.277657,perplexity:1.002994,cost-time:0.225196\n",
      "Step:1920/10262, training_loss:106.646271,perplexity:1.002780,cost-time:0.206660\n",
      "Step:1930/10262, training_loss:113.087654,perplexity:1.002933,cost-time:0.220877\n",
      "Step:1940/10262, training_loss:110.395767,perplexity:1.002848,cost-time:0.225182\n",
      "Step:1950/10262, training_loss:109.807388,perplexity:1.002818,cost-time:0.238249\n",
      "Step:1960/10262, training_loss:100.292191,perplexity:1.002560,cost-time:0.214821\n",
      "Step:1970/10262, training_loss:104.995819,perplexity:1.002667,cost-time:0.248160\n",
      "Step:1980/10262, training_loss:104.346252,perplexity:1.002637,cost-time:0.212857\n",
      "Step:1990/10262, training_loss:107.935257,perplexity:1.002714,cost-time:0.216965\n",
      "Step:2000/10262, training_loss:99.711494,perplexity:1.002495,cost-time:0.220698\n",
      "Step:2010/10262, training_loss:115.010468,perplexity:1.002864,cost-time:0.222047\n",
      "Step:2020/10262, training_loss:107.593353,perplexity:1.002665,cost-time:0.219142\n",
      "Step:2030/10262, training_loss:106.903915,perplexity:1.002635,cost-time:0.225584\n",
      "Step:2040/10262, training_loss:104.197601,perplexity:1.002556,cost-time:0.220700\n",
      "Step:2050/10262, training_loss:110.229805,perplexity:1.002691,cost-time:0.224201\n",
      "Step:2060/10262, training_loss:108.222572,perplexity:1.002629,cost-time:0.223351\n",
      "Step:2070/10262, training_loss:98.353523,perplexity:1.002377,cost-time:0.223930\n",
      "Step:2080/10262, training_loss:111.167580,perplexity:1.002675,cost-time:0.208647\n",
      "Step:2090/10262, training_loss:109.947105,perplexity:1.002633,cost-time:0.223334\n",
      "Step:2100/10262, training_loss:109.875183,perplexity:1.002618,cost-time:0.215305\n",
      "Step:2110/10262, training_loss:115.672501,perplexity:1.002744,cost-time:0.236051\n",
      "Step:2120/10262, training_loss:112.594131,perplexity:1.002658,cost-time:0.221658\n",
      "Step:2130/10262, training_loss:115.493507,perplexity:1.002714,cost-time:0.228788\n",
      "Step:2140/10262, training_loss:109.549698,perplexity:1.002562,cost-time:0.234608\n",
      "Step:2150/10262, training_loss:102.895851,perplexity:1.002395,cost-time:0.233636\n",
      "Step:2160/10262, training_loss:111.788216,perplexity:1.002590,cost-time:0.222989\n",
      "Step:2170/10262, training_loss:103.503242,perplexity:1.002387,cost-time:0.225865\n",
      "Step:2180/10262, training_loss:109.694191,perplexity:1.002518,cost-time:0.258200\n",
      "Step:2190/10262, training_loss:109.778908,perplexity:1.002508,cost-time:0.212528\n",
      "Step:2200/10262, training_loss:108.756050,perplexity:1.002474,cost-time:0.225578\n",
      "Step:2210/10262, training_loss:113.036812,perplexity:1.002560,cost-time:0.230266\n",
      "Step:2220/10262, training_loss:103.251579,perplexity:1.002327,cost-time:0.241243\n",
      "Step:2230/10262, training_loss:110.494019,perplexity:1.002479,cost-time:0.246384\n",
      "Step:2240/10262, training_loss:105.382812,perplexity:1.002354,cost-time:0.225549\n",
      "Step:2250/10262, training_loss:103.239410,perplexity:1.002296,cost-time:0.259452\n",
      "Step:2260/10262, training_loss:111.165672,perplexity:1.002461,cost-time:0.219318\n",
      "Step:2270/10262, training_loss:111.165466,perplexity:1.002450,cost-time:0.311026\n",
      "Step:2280/10262, training_loss:114.573074,perplexity:1.002515,cost-time:0.226961\n",
      "Step:2290/10262, training_loss:106.947685,perplexity:1.002337,cost-time:0.254064\n",
      "Step:2300/10262, training_loss:104.895615,perplexity:1.002282,cost-time:0.281239\n",
      "Step:2310/10262, training_loss:105.429977,perplexity:1.002284,cost-time:0.226302\n",
      "Step:2320/10262, training_loss:118.770691,perplexity:1.002562,cost-time:0.334569\n",
      "Step:2330/10262, training_loss:100.767967,perplexity:1.002164,cost-time:0.313944\n",
      "Step:2340/10262, training_loss:114.138718,perplexity:1.002441,cost-time:0.269514\n",
      "Step:2350/10262, training_loss:112.616959,perplexity:1.002398,cost-time:0.259758\n",
      "Step:2360/10262, training_loss:113.471588,perplexity:1.002406,cost-time:0.231126\n",
      "Step:2370/10262, training_loss:99.922623,perplexity:1.002109,cost-time:0.226784\n",
      "Step:2380/10262, training_loss:106.932541,perplexity:1.002248,cost-time:0.260899\n",
      "Step:2390/10262, training_loss:102.516335,perplexity:1.002146,cost-time:0.234862\n",
      "Step:2400/10262, training_loss:110.840004,perplexity:1.002311,cost-time:0.230910\n",
      "Step:2410/10262, training_loss:101.336708,perplexity:1.002104,cost-time:0.229656\n",
      "Step:2420/10262, training_loss:115.638733,perplexity:1.002391,cost-time:0.299859\n",
      "Step:2430/10262, training_loss:104.521561,perplexity:1.002152,cost-time:0.227412\n",
      "Step:2440/10262, training_loss:105.632858,perplexity:1.002166,cost-time:0.232964\n",
      "Step:2450/10262, training_loss:106.705467,perplexity:1.002179,cost-time:0.325811\n",
      "Step:2460/10262, training_loss:110.570137,perplexity:1.002249,cost-time:0.229849\n",
      "Step:2470/10262, training_loss:109.665039,perplexity:1.002222,cost-time:0.244463\n",
      "Step:2480/10262, training_loss:94.692932,perplexity:1.001910,cost-time:0.247353\n",
      "Step:2490/10262, training_loss:107.248413,perplexity:1.002155,cost-time:0.240052\n",
      "Step:2500/10262, training_loss:111.413422,perplexity:1.002230,cost-time:0.274994\n",
      "Step:2510/10262, training_loss:102.945465,perplexity:1.002052,cost-time:0.243134\n",
      "Step:2520/10262, training_loss:115.522530,perplexity:1.002294,cost-time:0.232274\n",
      "Step:2530/10262, training_loss:107.787857,perplexity:1.002132,cost-time:0.242549\n",
      "Step:2540/10262, training_loss:112.361954,perplexity:1.002213,cost-time:0.236449\n",
      "Step:2550/10262, training_loss:107.999222,perplexity:1.002119,cost-time:0.285487\n",
      "Step:2560/10262, training_loss:103.187286,perplexity:1.002017,cost-time:0.265800\n",
      "Step:2570/10262, training_loss:108.719330,perplexity:1.002117,cost-time:0.234069\n",
      "Step:2580/10262, training_loss:101.455322,perplexity:1.001967,cost-time:0.244103\n",
      "Step:2590/10262, training_loss:114.505051,perplexity:1.002212,cost-time:0.246190\n",
      "Step:2600/10262, training_loss:104.557976,perplexity:1.002012,cost-time:0.242886\n",
      "Step:2610/10262, training_loss:108.001740,perplexity:1.002070,cost-time:0.247275\n",
      "Step:2620/10262, training_loss:108.231247,perplexity:1.002067,cost-time:0.247181\n",
      "Step:2630/10262, training_loss:103.245079,perplexity:1.001964,cost-time:0.235229\n",
      "Step:2640/10262, training_loss:115.952133,perplexity:1.002198,cost-time:0.258111\n",
      "Step:2650/10262, training_loss:103.961700,perplexity:1.001963,cost-time:0.240843\n",
      "Step:2660/10262, training_loss:98.343254,perplexity:1.001850,cost-time:0.275588\n",
      "Step:2670/10262, training_loss:110.118713,perplexity:1.002064,cost-time:0.304182\n",
      "Step:2680/10262, training_loss:107.508987,perplexity:1.002007,cost-time:0.242049\n",
      "Step:2690/10262, training_loss:107.011864,perplexity:1.001990,cost-time:0.272055\n",
      "Step:2700/10262, training_loss:104.946747,perplexity:1.001945,cost-time:0.262525\n",
      "Step:2710/10262, training_loss:107.004234,perplexity:1.001975,cost-time:0.254471\n",
      "Step:2720/10262, training_loss:103.551697,perplexity:1.001905,cost-time:0.242582\n",
      "Step:2730/10262, training_loss:112.669159,perplexity:1.002065,cost-time:0.225843\n",
      "Step:2740/10262, training_loss:99.863785,perplexity:1.001823,cost-time:0.226396\n",
      "Step:2750/10262, training_loss:107.808617,perplexity:1.001961,cost-time:0.269008\n",
      "Step:2760/10262, training_loss:108.065628,perplexity:1.001959,cost-time:0.240389\n",
      "Step:2770/10262, training_loss:109.434891,perplexity:1.001977,cost-time:0.243332\n",
      "Step:2780/10262, training_loss:110.055313,perplexity:1.001981,cost-time:0.240755\n",
      "Step:2790/10262, training_loss:107.071693,perplexity:1.001920,cost-time:0.233483\n",
      "Step:2800/10262, training_loss:102.432945,perplexity:1.001830,cost-time:0.233607\n",
      "Step:2810/10262, training_loss:109.591995,perplexity:1.001951,cost-time:0.222344\n",
      "Step:2820/10262, training_loss:107.920624,perplexity:1.001915,cost-time:0.255126\n",
      "Step:2830/10262, training_loss:113.279915,perplexity:1.002003,cost-time:0.234937\n",
      "Step:2840/10262, training_loss:102.361145,perplexity:1.001803,cost-time:0.240289\n",
      "Step:2850/10262, training_loss:107.653679,perplexity:1.001890,cost-time:0.233143\n",
      "Step:2860/10262, training_loss:107.639748,perplexity:1.001883,cost-time:0.258114\n",
      "Step:2870/10262, training_loss:119.004730,perplexity:1.002075,cost-time:0.244495\n",
      "Step:2880/10262, training_loss:105.607887,perplexity:1.001835,cost-time:0.227293\n",
      "Step:2890/10262, training_loss:103.536667,perplexity:1.001792,cost-time:0.266022\n",
      "Step:2900/10262, training_loss:99.698288,perplexity:1.001720,cost-time:0.235127\n",
      "Step:2910/10262, training_loss:106.447525,perplexity:1.001830,cost-time:0.223875\n",
      "Step:2920/10262, training_loss:103.264572,perplexity:1.001769,cost-time:0.265472\n",
      "Step:2930/10262, training_loss:109.656601,perplexity:1.001872,cost-time:0.234060\n",
      "Step:2940/10262, training_loss:106.626404,perplexity:1.001814,cost-time:0.218000\n",
      "Step:2950/10262, training_loss:105.221817,perplexity:1.001784,cost-time:0.226775\n",
      "Step:2960/10262, training_loss:109.147018,perplexity:1.001845,cost-time:0.233574\n",
      "Step:2970/10262, training_loss:104.642227,perplexity:1.001763,cost-time:0.211084\n",
      "Step:2980/10262, training_loss:104.864021,perplexity:1.001760,cost-time:0.223827\n",
      "Step:2990/10262, training_loss:101.005234,perplexity:1.001690,cost-time:0.238900\n",
      "Step:3000/10262, training_loss:119.495743,perplexity:1.001993,cost-time:0.213136\n",
      "Step:3010/10262, training_loss:99.074623,perplexity:1.001647,cost-time:0.239689\n",
      "Step:3020/10262, training_loss:103.374825,perplexity:1.001712,cost-time:0.247945\n",
      "Step:3030/10262, training_loss:105.994087,perplexity:1.001750,cost-time:0.280620\n",
      "Step:3040/10262, training_loss:100.620323,perplexity:1.001656,cost-time:0.231165\n",
      "Step:3050/10262, training_loss:109.950546,perplexity:1.001804,cost-time:0.278879\n",
      "Step:3060/10262, training_loss:107.350662,perplexity:1.001755,cost-time:0.245468\n",
      "Step:3070/10262, training_loss:99.286865,perplexity:1.001618,cost-time:0.235656\n",
      "Step:3080/10262, training_loss:104.785011,perplexity:1.001702,cost-time:0.263438\n",
      "Step:3090/10262, training_loss:104.418228,perplexity:1.001690,cost-time:0.231340\n",
      "Step:3100/10262, training_loss:96.836914,perplexity:1.001563,cost-time:0.279705\n",
      "Step:3110/10262, training_loss:107.133705,perplexity:1.001723,cost-time:0.234795\n",
      "Step:3120/10262, training_loss:104.011719,perplexity:1.001668,cost-time:0.255693\n",
      "Step:3130/10262, training_loss:104.681351,perplexity:1.001673,cost-time:0.259990\n",
      "Step:3140/10262, training_loss:101.448380,perplexity:1.001616,cost-time:0.233927\n",
      "Step:3150/10262, training_loss:110.846497,perplexity:1.001760,cost-time:0.215362\n",
      "Step:3160/10262, training_loss:103.027939,perplexity:1.001631,cost-time:0.262565\n",
      "Step:3170/10262, training_loss:96.747673,perplexity:1.001527,cost-time:0.265340\n",
      "Step:3180/10262, training_loss:108.746140,perplexity:1.001711,cost-time:0.250283\n",
      "Step:3190/10262, training_loss:109.565422,perplexity:1.001718,cost-time:0.229207\n",
      "Step:3200/10262, training_loss:107.680283,perplexity:1.001683,cost-time:0.225146\n",
      "Step:3210/10262, training_loss:101.832840,perplexity:1.001587,cost-time:0.283335\n",
      "Step:3220/10262, training_loss:106.377182,perplexity:1.001653,cost-time:0.284277\n",
      "Step:3230/10262, training_loss:111.114334,perplexity:1.001721,cost-time:0.238718\n",
      "Step:3240/10262, training_loss:106.646454,perplexity:1.001647,cost-time:0.257053\n",
      "Step:3250/10262, training_loss:97.629593,perplexity:1.001503,cost-time:0.227727\n",
      "Step:3260/10262, training_loss:107.589493,perplexity:1.001651,cost-time:0.217943\n",
      "Step:3270/10262, training_loss:105.166504,perplexity:1.001609,cost-time:0.215097\n",
      "Step:3280/10262, training_loss:114.010330,perplexity:1.001739,cost-time:0.247751\n",
      "Step:3290/10262, training_loss:102.426224,perplexity:1.001557,cost-time:0.247475\n",
      "Step:3300/10262, training_loss:107.692947,perplexity:1.001633,cost-time:0.249978\n",
      "Step:3310/10262, training_loss:93.402756,perplexity:1.001411,cost-time:0.280203\n",
      "Step:3320/10262, training_loss:100.513229,perplexity:1.001514,cost-time:0.271920\n",
      "Step:3330/10262, training_loss:105.860321,perplexity:1.001590,cost-time:0.247039\n",
      "Step:3340/10262, training_loss:97.109535,perplexity:1.001454,cost-time:0.244693\n",
      "Step:3350/10262, training_loss:104.483246,perplexity:1.001560,cost-time:0.223513\n",
      "Step:3360/10262, training_loss:105.100769,perplexity:1.001565,cost-time:0.252311\n",
      "Step:3370/10262, training_loss:109.560562,perplexity:1.001626,cost-time:0.267268\n",
      "Step:3380/10262, training_loss:102.112717,perplexity:1.001511,cost-time:0.221953\n",
      "Step:3390/10262, training_loss:100.825073,perplexity:1.001488,cost-time:0.235668\n",
      "Step:3400/10262, training_loss:101.303230,perplexity:1.001490,cost-time:0.228336\n",
      "Step:3410/10262, training_loss:107.726257,perplexity:1.001580,cost-time:0.226354\n",
      "Step:3420/10262, training_loss:96.237984,perplexity:1.001408,cost-time:0.226988\n",
      "Step:3430/10262, training_loss:99.949913,perplexity:1.001458,cost-time:0.237082\n",
      "Step:3440/10262, training_loss:99.419662,perplexity:1.001446,cost-time:0.226131\n",
      "Step:3450/10262, training_loss:97.974792,perplexity:1.001421,cost-time:0.261168\n",
      "Step:3460/10262, training_loss:102.861954,perplexity:1.001487,cost-time:0.282651\n",
      "Step:3470/10262, training_loss:103.147232,perplexity:1.001487,cost-time:0.229376\n",
      "Step:3480/10262, training_loss:104.771881,perplexity:1.001506,cost-time:0.239537\n",
      "Step:3490/10262, training_loss:98.319069,perplexity:1.001409,cost-time:0.247917\n",
      "Step:3500/10262, training_loss:98.758987,perplexity:1.001411,cost-time:0.273334\n",
      "Step:3510/10262, training_loss:93.598923,perplexity:1.001334,cost-time:0.290431\n",
      "Step:3520/10262, training_loss:106.816559,perplexity:1.001518,cost-time:0.231317\n",
      "Step:3530/10262, training_loss:93.990326,perplexity:1.001332,cost-time:0.310413\n",
      "Step:3540/10262, training_loss:97.533737,perplexity:1.001378,cost-time:0.235180\n",
      "Step:3550/10262, training_loss:100.720932,perplexity:1.001419,cost-time:0.231374\n",
      "Step:3560/10262, training_loss:108.919556,perplexity:1.001531,cost-time:0.257153\n",
      "Step:3570/10262, training_loss:97.817169,perplexity:1.001371,cost-time:0.263892\n",
      "Step:3580/10262, training_loss:98.982071,perplexity:1.001383,cost-time:0.224704\n",
      "Step:3590/10262, training_loss:110.275826,perplexity:1.001537,cost-time:0.224686\n",
      "Step:3600/10262, training_loss:104.597672,perplexity:1.001453,cost-time:0.240254\n",
      "Step:3610/10262, training_loss:106.617416,perplexity:1.001477,cost-time:0.243294\n",
      "Step:3620/10262, training_loss:101.855919,perplexity:1.001407,cost-time:0.230931\n",
      "Step:3630/10262, training_loss:105.496643,perplexity:1.001454,cost-time:0.232792\n",
      "Step:3640/10262, training_loss:107.147316,perplexity:1.001472,cost-time:0.250428\n",
      "Step:3650/10262, training_loss:105.339111,perplexity:1.001444,cost-time:0.221285\n",
      "Step:3660/10262, training_loss:101.608620,perplexity:1.001389,cost-time:0.251867\n",
      "Step:3670/10262, training_loss:100.513596,perplexity:1.001370,cost-time:0.248926\n",
      "Step:3680/10262, training_loss:102.340576,perplexity:1.001391,cost-time:0.229431\n",
      "Step:3690/10262, training_loss:104.937920,perplexity:1.001423,cost-time:0.228947\n",
      "Step:3700/10262, training_loss:105.174881,perplexity:1.001422,cost-time:0.229576\n",
      "Step:3710/10262, training_loss:102.833000,perplexity:1.001386,cost-time:0.238469\n",
      "Step:3720/10262, training_loss:93.088074,perplexity:1.001252,cost-time:0.219589\n",
      "Step:3730/10262, training_loss:98.099823,perplexity:1.001316,cost-time:0.262245\n",
      "Step:3740/10262, training_loss:102.143494,perplexity:1.001366,cost-time:0.249101\n",
      "Step:3750/10262, training_loss:98.807159,perplexity:1.001318,cost-time:0.258982\n",
      "Step:3760/10262, training_loss:101.518646,perplexity:1.001351,cost-time:0.256706\n",
      "Step:3770/10262, training_loss:105.290932,perplexity:1.001397,cost-time:0.337994\n",
      "Step:3780/10262, training_loss:104.159134,perplexity:1.001378,cost-time:0.233649\n",
      "Step:3790/10262, training_loss:96.625801,perplexity:1.001275,cost-time:0.234349\n",
      "Step:3800/10262, training_loss:100.351410,perplexity:1.001321,cost-time:0.226262\n",
      "Step:3810/10262, training_loss:100.723289,perplexity:1.001322,cost-time:0.277337\n",
      "Step:3820/10262, training_loss:98.564247,perplexity:1.001291,cost-time:0.232891\n",
      "Step:3830/10262, training_loss:96.710205,perplexity:1.001263,cost-time:0.217465\n",
      "Step:3840/10262, training_loss:102.021393,perplexity:1.001329,cost-time:0.261085\n",
      "Step:3850/10262, training_loss:98.924248,perplexity:1.001285,cost-time:0.226673\n",
      "Step:3860/10262, training_loss:102.314301,perplexity:1.001326,cost-time:0.204737\n",
      "Step:3870/10262, training_loss:97.341583,perplexity:1.001258,cost-time:0.255412\n",
      "Step:3880/10262, training_loss:103.344460,perplexity:1.001332,cost-time:0.206394\n",
      "Step:3890/10262, training_loss:104.678169,perplexity:1.001346,cost-time:0.222640\n",
      "Step:3900/10262, training_loss:98.422195,perplexity:1.001262,cost-time:0.231120\n",
      "Step:3910/10262, training_loss:89.332932,perplexity:1.001143,cost-time:0.229021\n",
      "Step:3920/10262, training_loss:95.543716,perplexity:1.001219,cost-time:0.224929\n",
      "Step:3930/10262, training_loss:98.798004,perplexity:1.001257,cost-time:0.220631\n",
      "Step:3940/10262, training_loss:93.105560,perplexity:1.001182,cost-time:0.291783\n",
      "Step:3950/10262, training_loss:93.924660,perplexity:1.001189,cost-time:0.249612\n",
      "Step:3960/10262, training_loss:105.262985,perplexity:1.001330,cost-time:0.328032\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9d335f479abd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-686f54fe2b5a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(is_training)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrun_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_training\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-5f63753991b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, config)\u001b[0m\n\u001b[1;32m     24\u001b[0m                          model.target_data: y_batch, model.lr: learning_rate}\n\u001b[1;32m     25\u001b[0m             train_loss, summary, _, _ = sess.run([model.cost, model.merged_op, model.last_state, model.train_op],\n\u001b[0;32m---> 26\u001b[0;31m                                                  feed_dict)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# ------------------------ #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
